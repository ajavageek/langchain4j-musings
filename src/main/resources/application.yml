spring.application.name: langchain4j-musings
langchain4j.ollama.streaming-chat-model:
  log-requests: true
  log-responses: true
  base-url: http://localhost:11434
  model-name: llama3.2
